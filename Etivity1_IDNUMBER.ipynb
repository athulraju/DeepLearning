{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-tivity 1 (Weeks 1-2)\n",
    "\n",
    "* Your Name\n",
    "\n",
    "* Your Student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "### Context\n",
    "We have a mystery dataset. There are 9 explanatory variables and one response variable. The response variable is the last column and indicates if the sample is anomalous (=1, valid =0). The dataset is provided \"data.csv\". \n",
    "\n",
    "Of course in this case we could use supervised learning to generate a model and detect anomalies in new data. However the focus is on autoencoders, anomaly detection is just one of the potential uses for autoencoders.\n",
    "\n",
    "So we are going to pretend that we do not know which data are anomalous but we do know that the anomaly rate is small. Use an autoencoder to detect anomalies in the data. The correctness of the model can of course be checked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "\n",
    "The e-tivity is split into four tasks. The first three are \"group\" excersises, in that you post the solutions to Tasks 1-3 to a site. This will allow the members of your group to send you feedback (via the forums) so you can improve your submission. The final task is an individual task and together with the other tasks, should be uploaded to the Group Locker. \n",
    "\n",
    "Marks will be deducted if task 4 is uploaded in contravention of instructions. Also if the the final submission is not a single notebook with tasks 1-4 and with correct identification or filename.\n",
    "\n",
    "Grading guidelines: the scores for each task are additive. Below is a link to the E-tivity rubrics.\n",
    "\n",
    "https://learn.ul.ie/d2l/lp/rubrics/preview.d2l?ou=73310&rubricId=4445&originTool=quicklinks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "If you train even a modest feed forward network via supervised learning you can get extremely good recall and precision, despite the unbalanced dataset. However in this e-tivity you will determining the anomalies by using an autoencoder. That is you will **not** be using the Anom flag to guide the training.\n",
    "\n",
    "The mystery dataset is available from Brightspace, download the csv file and use it as the input data.\n",
    "\n",
    "### Tasks 1-3 (complete by Sunday Week 2)\n",
    "\n",
    "Create additional cells to implement the tasks below. These task form the group learning section of the E-tivity. As such it is encouraged that you post yuor notebook in the group locker. \n",
    "\n",
    "Grading guidelines are given in the rubrics for the E-tivity\n",
    "\n",
    "https://learn.ul.ie/d2l/lp/rubrics/preview.d2l?ou=73310&rubricId=4445&originTool=quicklinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: data preprocessing**\n",
    "\n",
    "Explain any preprocessing steps you take and also how you have selected the training and test sets. Remember we do not know which samples are anomalous only that there are a small number of them compared to the total sample size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "X = df.iloc[:, :-1].values   # 9 features\n",
    "y = df.iloc[:, -1].values   # anom feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49097, 10)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'Anom'], dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49097 entries, 0 to 49096\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   V1      49097 non-null  int64\n",
      " 1   V2      49097 non-null  int64\n",
      " 2   V3      49097 non-null  int64\n",
      " 3   V4      49097 non-null  int64\n",
      " 4   V5      49097 non-null  int64\n",
      " 5   V6      49097 non-null  int64\n",
      " 6   V7      49097 non-null  int64\n",
      " 7   V8      49097 non-null  int64\n",
      " 8   V9      49097 non-null  int64\n",
      " 9   Anom    49097 non-null  int64\n",
      "dtypes: int64(10)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column: V1\n",
      "count    49097.000000\n",
      "mean        46.932399\n",
      "std         12.875159\n",
      "min         27.000000\n",
      "25%         37.000000\n",
      "50%         44.000000\n",
      "75%         50.000000\n",
      "max        126.000000\n",
      "Name: V1, dtype: float64\n",
      "\n",
      "Column: V2\n",
      "count    49097.000000\n",
      "mean        -0.063955\n",
      "std         84.674481\n",
      "min      -4821.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max       5075.000000\n",
      "Name: V2, dtype: float64\n",
      "\n",
      "Column: V3\n",
      "count    49097.000000\n",
      "mean        85.123124\n",
      "std          8.877517\n",
      "min         21.000000\n",
      "25%         79.000000\n",
      "50%         83.000000\n",
      "75%         88.000000\n",
      "max        149.000000\n",
      "Name: V3, dtype: float64\n",
      "\n",
      "Column: V4\n",
      "count    49097.000000\n",
      "mean         0.213231\n",
      "std         37.579171\n",
      "min      -3939.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max       3830.000000\n",
      "Name: V4, dtype: float64\n",
      "\n",
      "Column: V5\n",
      "count    49097.000000\n",
      "mean        36.871784\n",
      "std         19.963113\n",
      "min       -188.000000\n",
      "25%         30.000000\n",
      "50%         42.000000\n",
      "75%         46.000000\n",
      "max        436.000000\n",
      "Name: V5, dtype: float64\n",
      "\n",
      "Column: V6\n",
      "count    49097.000000\n",
      "mean         2.160030\n",
      "std        218.324964\n",
      "min     -26739.000000\n",
      "25%         -4.000000\n",
      "50%          0.000000\n",
      "75%          5.000000\n",
      "max      15164.000000\n",
      "Name: V6, dtype: float64\n",
      "\n",
      "Column: V7\n",
      "count    49097.000000\n",
      "mean        38.200725\n",
      "std         13.446306\n",
      "min        -48.000000\n",
      "25%         33.000000\n",
      "50%         39.000000\n",
      "75%         43.000000\n",
      "max        105.000000\n",
      "Name: V7, dtype: float64\n",
      "\n",
      "Column: V8\n",
      "count    49097.000000\n",
      "mean        48.288592\n",
      "std         20.572064\n",
      "min       -353.000000\n",
      "25%         35.000000\n",
      "50%         41.000000\n",
      "75%         55.000000\n",
      "max        270.000000\n",
      "Name: V8, dtype: float64\n",
      "\n",
      "Column: V9\n",
      "count    49097.000000\n",
      "mean        10.261930\n",
      "std         23.751024\n",
      "min       -356.000000\n",
      "25%          0.000000\n",
      "50%          2.000000\n",
      "75%          6.000000\n",
      "max        266.000000\n",
      "Name: V9, dtype: float64\n",
      "\n",
      "Column: Anom\n",
      "count    49097.000000\n",
      "mean         0.071511\n",
      "std          0.257680\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          1.000000\n",
      "Name: Anom, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(df[col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 50,  21,  77, ...,  27,  48,  22],\n",
       "        [ 53,   0,  82, ...,  29,  30,   2],\n",
       "        [ 37,   0,  76, ...,  40,  48,   8],\n",
       "        ...,\n",
       "        [ 49,   0,  87, ...,  38,  41,   2],\n",
       "        [ 80,   0,  84, ...,   4, 120, 116],\n",
       "        [ 37,   0, 103, ...,  66,  85,  20]]),\n",
       " array([1, 0, 0, ..., 0, 1, 0]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using training data only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled features to pytorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_tensor),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: model generation and training**\n",
    "\n",
    "Generate a suitable autoencoder model, the only restriction is that there should be only 2 latent variables. Train the model to a satistifactory result. Be aware that it will be much harder to achieve the sort of result you can get from a supervised learning model. \n",
    "\n",
    "**Hint**: it should not take longer than a 1000 epochs to train. However it may be difficult to train. Use different optimizers, topologies and/or weight initialisations to get convergence. Remember that achieving a perfect error means that the model will also be good at reconstructing anomalies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(9, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 2)  # latent space\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 9)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.604302\n",
      "Epoch 50, Loss: 0.179380\n",
      "Epoch 100, Loss: 0.154465\n",
      "Epoch 150, Loss: 0.150858\n",
      "Epoch 200, Loss: 0.148905\n",
      "Epoch 250, Loss: 0.145386\n",
      "Epoch 300, Loss: 0.144945\n",
      "Epoch 350, Loss: 0.111146\n",
      "Epoch 400, Loss: 0.072077\n",
      "Epoch 450, Loss: 0.071097\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for (batch,) in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon = model(batch)\n",
    "        loss = criterion(recon, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    losses.append(epoch_loss / len(train_loader))\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: anomaly detection**\n",
    "\n",
    "From the histogram of the reconstruction error decide what the cutoff should be applied to distinguish anomalies from valid samples, given that the anomaly rate is ~7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post (complete by Sunday Week 1)\n",
    "\n",
    "Once you have an implemntation however rough, post it to the grouo loocker and use the forums/topics to advertise the fact. The purpose is to get feedback from others in the group, so if you have only a basic outline then you may get ideas about how to proceed and also examples from others in your group.\n",
    "\n",
    "No posts should refer to Task 4.\n",
    "\n",
    "This forms part of the overall assessment for the E-tivity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respond (complete by Wednesday Week 2)\n",
    "\n",
    "If you feel you can provide useful advise then respond to another member of the group through the appropriate forum. Responses should be respectful and offer some sort of advise. Try and avoid clogging the forums with support or thank you messages.\n",
    "\n",
    "In reviewing others code you will discover different ways to tackle the same problem. It is acceptable to copy parts of others code. However whole scale copying from another notebook is not acceptable.\n",
    "\n",
    "If you stick to the deadline then it will allow yourself and others to have enough time to implement suggestions. From the Schedule you can see that this is not a hard deadline.\n",
    "\n",
    "The posts on the forum/topic associated with this e-tivity are graded. Below is a link to the rubrics.\n",
    "\n",
    "https://learn.ul.ie/d2l/lp/rubrics/preview.d2l?ou=73310&rubricId=4445&originTool=quicklinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: VAE (completed by Sunday Week 2)\n",
    "\n",
    "This task is a individual task and should **not** to be uploaded to the Group Locker. No direct support should be given via the forums. Marks will be deducted if the instructions are not followed (see rubrics). This part should be uploaded directly to Brightpsace.\n",
    "\n",
    "Change the network to be a VAE. Again determine the optimal cutoff and plot the latent variables. Check how good the cutoffs were by constructing a confusion matrix or generating a classification report. Obviously for this task you need to use the Anom column.\n",
    "\n",
    "**Hint** you can use the model topology from the AE (with the obvious modifications). I found that I had a good model (almost as good and the supervised learning model) when the KL divergence was small. You can print out both the KL divergence and reconstruction loss for each epoch. It can be tricky to train these type of models, so do not be surprised if you do not get a stellar result. What is more important is that you have the correct code to implement the VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Submission (complete by Sunday Week 2)\n",
    "\n",
    "Submit Tasks 1-4 in a single notebook this before the deadline on Sunday.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add additional code cells to implememt the tasks stated above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "There are no specific marks allocated for a reflection. However due consideration will be given if pertinent comments or valuable insights are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
